{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preconditioning\n",
    "\n",
    "\n",
    "## Nordic Graduate Coarse on Computational Mathematical Modeling\n",
    "### Miroslav Kuchta (miroslav.kuchta@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains set of exercises accompanying the lecture. Prerequisites for executing the code are working installations of [FEniCS](https://fenicsproject.org/download/) and [cbc.block](https://bitbucket.org/fenics-apps/cbc.block). The lecture material and some of the exercises were inspired by the following books and papers (I tried to list them in the order of relavance)\n",
    "\n",
    "1. [Preconditioning discretizations of systems of partial differential equations](http://onlinelibrary.wiley.com/doi/10.1002/nla.716/abstract)\n",
    "2. [Preconditioning and the Conjugate Gradient Method in the Context of Solving PDEs](http://epubs.siam.org/doi/book/10.1137/1.9781611973846)\n",
    "3. [From Functional Analysis to Iterative Methods](http://epubs.siam.org/doi/abs/10.1137/070706914)\n",
    "4. [A Note on Preconditioners and Scalar Products in Krylov Subspace Methods for Self-adjoint Problems in Hilbert Space ](etna.mcs.kent.edu/vol.41.2014/pp13-20.dir/pp13-20.pdf)\n",
    "5. [The Mathematical Theory of Finite Element Methods](https://books.google.no/books/about/The_Mathematical_Theory_of_Finite_Elemen.html?id=ci4c_R0WKYYC&redir_esc=y)\n",
    "6. [An Introduction to the Conjugate Gradient Method Without the Agonizing Pain](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)\n",
    "7. [A Multigrid Tutorial](http://epubs.siam.org/doi/book/10.1137/1.9780898719505)\n",
    "8. [Realistic Eigenvalue Bounds for the Galerkin Mass Matrix](https://academic.oup.com/imajna/article-abstract/7/4/449/841299/Realistic-Eigenvalue-Bounds-for-the-Galerkin-Mass?redirectedFrom=PDF)\n",
    "\n",
    "In addition [Golub's](https://jhupbooks.press.jhu.edu/content/matrix-computations) book covers many more classical iterative and Krylov subspace methods that were not covered in the lecture. [Strang](http://epubs.siam.org/doi/abs/10.1137/1028182)'s book has a very readable introduction to multigrid while [Quarteroni](http://www.springer.com/gp/book/9788847055216)'s book is a nice read on domain decomposition techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling of the stiffness matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dolfin import *\n",
    "\n",
    "def HyperCubeMesh(dim, ncells):\n",
    "    return {1: UnitIntervalMesh,\n",
    "            2: UnitSquareMesh,\n",
    "            3: UnitCubeMesh}[dim](*(ncells, )*dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our prediction that the condition number of the stiffness matrix is $\\mathcal{O}(h^{-2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stiffness_cond(dim, n):\n",
    "    '''\n",
    "    Condition number of matrix corresponding to\n",
    "    \n",
    "        -\\Delta u = f in [0, 1]^dim\n",
    "                u = 0 on boundary\n",
    "    \n",
    "    discretized with P1 elements.\n",
    "    '''\n",
    "    mesh = HyperCubeMesh(dim, n)\n",
    "    # Want all eigenvalues so keep it small\n",
    "    assert mesh.num_vertices() < 5000 \n",
    "    \n",
    "    mesh = HyperCubeMesh(dim, n)\n",
    "    h = mesh.hmin() \n",
    "    \n",
    "    V = FunctionSpace(mesh, 'CG', 1)\n",
    "    u = TrialFunction(V)\n",
    "    v = TestFunction(V)\n",
    "    \n",
    "    a = inner(grad(u), grad(v))*dx\n",
    "    L = inner(Constant(0), v)*dx\n",
    "    bc = DirichletBC(V, Constant(0), 'on_boundary')\n",
    "    \n",
    "    A, _ = assemble_system(a, L, bc)\n",
    "    A = A.array()\n",
    "    \n",
    "    lmin, lmax = np.sort(np.linalg.eigvalsh(A))[[0, -1]]\n",
    "    \n",
    "    return h, lmax/lmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (14, 8)\n",
    "matplotlib.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dim = 1\n",
    "h, cond = np.array([stiffness_cond(1, 2**i) for i in range(2, 11)]).T\n",
    "\n",
    "plt.loglog(h, cond, label='$\\kappa$')\n",
    "plt.loglog(h, h**(-2), label='$h^{-2}$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative solvers for Poisson problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poisson(dim, n, sigma=Constant(1), epsilon=Constant(1)):\n",
    "    '''\n",
    "    Linear system corresponding to P1 discretization of\n",
    "    \n",
    "        -\\Delta u + sigma*u = f in [0, 1]^dim\n",
    "                          u = 0 on D = {x = boundary, x[0] == 0}\n",
    "          du/dn + epsilon*u = 0 on boundary\\D\n",
    "    '''\n",
    "    mesh = HyperCubeMesh(dim, n)\n",
    "    \n",
    "    V = FunctionSpace(mesh, 'CG', 1)\n",
    "    u = TrialFunction(V)\n",
    "    v = TestFunction(V)\n",
    "    \n",
    "    bc = DirichletBC(V, Constant(0), 'near(x[0], 0)')\n",
    "    boundaries = FacetFunction('size_t', mesh, 1)\n",
    "    # So that Neumann is the rest, 1\n",
    "    CompiledSubDomain('near(x[0], 0)').mark(boundaries, 0)\n",
    "    # Measure for Neumann\n",
    "    dsN = Measure('ds', domain=mesh, subdomain_data=boundaries, subdomain_id=1)\n",
    "    \n",
    "    a = inner(grad(u), grad(v))*dx + inner(sigma*u, v)*dx + inner(epsilon*u, v)*dsN\n",
    "    L = inner(Constant(0), v)*dx\n",
    "    \n",
    "    A, b = assemble_system(a, L, bc)\n",
    "    \n",
    "    return A, b, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We showed the the above problem is well-posed with $V=H^1_{0, \\Gamma_D}$ considered with both the $H^1$ norm\n",
    "or the $H^1$ seminorm. Let's define the corresponding Riesz maps. Note that we will use algebraic multigrid to define\n",
    "their approximate inverse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- Try other ways to approximate the mapping. ML, ILU, [etc](https://bitbucket.org/fenics-apps/cbc.block/src/master/block/algebraic/petsc/precond.py?at=master&fileviewer=file-view-default). Look at iteration count as well as the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from block.algebraic.petsc import AMG\n",
    "\n",
    "def riesz_H1(V):\n",
    "    '''Operator corresponding to approx. Riesz map w.r.t to H1 norm. '''\n",
    "    u = TrialFunction(V)\n",
    "    v = TestFunction(V)\n",
    "    \n",
    "    bc = DirichletBC(V, Constant(0), 'near(x[0], 0)')\n",
    "    a = inner(grad(u), grad(v))*dx + inner(u, v)*dx\n",
    "    L = inner(Constant(0), v)*dx\n",
    "    \n",
    "    A, _ = assemble_system(a, L, bc)\n",
    "    return AMG(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def riesz_H10(V):\n",
    "    '''Operator corresponding to approx. Riesz map w.r.t to H1 seminorm. '''\n",
    "    u = TrialFunction(V)\n",
    "    v = TestFunction(V)\n",
    "    \n",
    "    bc = DirichletBC(V, Constant(0), 'near(x[0], 0)')\n",
    "    a = inner(grad(u), grad(v))*dx\n",
    "    L = inner(Constant(0), v)*dx\n",
    "    \n",
    "    A, _ = assemble_system(a, L, bc)\n",
    "    return AMG(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use implementation of Richardson algorithm from cbc.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from block.iterative import Richardson\n",
    "\n",
    "riesz = riesz_H10\n",
    "\n",
    "dim = 2\n",
    "for n in (8, 16, 32, 64, 128, 256, 512):\n",
    "    A, b, V = poisson(dim, n)\n",
    "    B = riesz(V)\n",
    "    \n",
    "    uh = Function(V)\n",
    "    x = uh.vector()\n",
    "    # Start from random initial guess\n",
    "    x.set_local(np.random.rand(x.local_size())) \n",
    "    \n",
    "    # Step size \\rho\n",
    "    rho = 0.5\n",
    "    Ainv = Richardson(A=A, precond=rho*B, initial_guess=x, tolerance=1e-4)\n",
    "    x[:] = Ainv*b\n",
    "    niters = len(Ainv.residuals)-1\n",
    "    \n",
    "    print 'n = %d, dofs = %d, niters = %d' % (n, A.size(0), niters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exericise**\n",
    "- How does parameter of algorithm $\\rho$ effect convergence behavior?\n",
    "- Try setting `B=1`, i.e. no preconditioner. What happens?\n",
    "- What happens when you change the values of problem parameters $\\sigma$ and $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Jacobi? This method is studied in great detail in reference [7] together with its weighted version. In short, stiffness matrices do not have the right spectral properties for it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from petsc4py import PETSc\n",
    "\n",
    "def Diag(A):\n",
    "    '''Diagonal matrix diag(A)'''\n",
    "    dvec = as_backend_type(A).mat().getDiagonal()\n",
    "\n",
    "    diagAinv = PETSc.Mat().createAIJ(dvec.size, nnz=1)\n",
    "    diagAinv.setDiagonal(dvec)\n",
    "    return PETScMatrix(diagAinv)\n",
    "\n",
    "\n",
    "def InvDiag(A):\n",
    "    '''Diagonal matrix diag(A)^-1'''\n",
    "    dvec = as_backend_type(A).mat().getDiagonal()\n",
    "    dvec.reciprocal()\n",
    "\n",
    "    diagAinv = PETSc.Mat().createAIJ(dvec.size, nnz=1)\n",
    "    diagAinv.setDiagonal(dvec)\n",
    "    return PETScMatrix(diagAinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim = 1\n",
    "for n in (8, 16, 32, 64, 128, 256, 512):\n",
    "    A, b, V = poisson(dim, n)\n",
    "    B = InvDiag(A)\n",
    "    \n",
    "    uh = Function(V)\n",
    "    x = uh.vector()\n",
    "    # Start from random initial guess\n",
    "    x.set_local(np.random.rand(x.local_size())) \n",
    "    \n",
    "    Ainv = Richardson(A=A, precond=2./3*B, initial_guess=x, tolerance=1e-4)\n",
    "    x[:] = Ainv*b\n",
    "    niters = len(Ainv.residuals)-1\n",
    "    \n",
    "    print 'n = %d, dofs = %d, niters = %d' % (n, A.size(0), niters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- Using what you heard on the lecture and and reference [8] do you expect Jacobi to work for the problem of finding the $L^2$ projection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def projection(dim, n):\n",
    "    '''\n",
    "    Linear system corresponding to L^2 projection\n",
    "    '''\n",
    "    mesh = HyperCubeMesh(dim, n)\n",
    "    \n",
    "    V = FunctionSpace(mesh, 'CG', 1)\n",
    "    u = TrialFunction(V)\n",
    "    v = TestFunction(V)\n",
    "    \n",
    "    a = inner(u, v)*dx\n",
    "    L = inner(Constant(1), v)*dx\n",
    "    \n",
    "    A, b = assemble_system(a, L)\n",
    "    \n",
    "    return A, b, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import eigvalsh\n",
    "\n",
    "dim = 3\n",
    "for n in (2, 4, 8, 16, 32):\n",
    "    M, b, V = projection(dim, n)\n",
    "    B = InvDiag(M)\n",
    "    \n",
    "    # Look at the condition number of the system\n",
    "    if dim < 3 or V.dim() < 5000:\n",
    "        Binv = Diag(M)\n",
    "        lmin, lmax = np.sort(eigvalsh(M.array(), Binv.array()))[[0, -1]]\n",
    "        print 'lmin = %g, lmax = %g, cond = %g' % (lmin, lmax, lmax/lmin)\n",
    "    \n",
    "    uh = Function(V)\n",
    "    x = uh.vector()\n",
    "    # Start from smooth initial guess, 0\n",
    "    # Note the scaling 2/3.\n",
    "    Ainv = Richardson(A=M, precond=2./3*B, initial_guess=x, tolerance=1e-4, maxiter=10000)\n",
    "    x[:] = Ainv*b\n",
    "    niters = len(Ainv.residuals)-1\n",
    "    \n",
    "    print (dim<3 or V.dim() < 5000)*'\\t', 'n = %d, dofs = %d, niters = %d' % (n, M.size(0), niters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see about some parameter free methods. We begin with steepest descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def steepest_descent(A, x, b, B, tolerance, maxiter=200):\n",
    "    '''Preconditioned steepest descent method.'''\n",
    "    r = b - A*x\n",
    "    error = sqrt(r.inner(r))\n",
    "\n",
    "    iter = 0\n",
    "    while error > tolerance and iter < maxiter:\n",
    "        \n",
    "        p = B*r\n",
    "        Ap = A*p \n",
    "        rho = r.inner(p)/Ap.inner(p)\n",
    "        x += rho*p\n",
    "        r = b - A*x\n",
    "\n",
    "        error = sqrt(r.inner(r))\n",
    "        iter += 1\n",
    "\n",
    "    return iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riesz = riesz_H10\n",
    "\n",
    "dim = 2\n",
    "for n in (8, 16, 32, 64, 128, 256, 512):\n",
    "    A, b, V = poisson(dim, n)\n",
    "    B = riesz(V)\n",
    "    \n",
    "    uh = Function(V)\n",
    "    x = uh.vector()\n",
    "    # Start from random initial guess\n",
    "    x.set_local(np.random.rand(x.local_size())) \n",
    "    \n",
    "    niters = steepest_descent(A, x, b, B, tolerance=1e-4, maxiter=200)\n",
    "    \n",
    "    print 'n = %d, dofs = %d, niters = %d' % (n, A.size(0), niters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we shall see about the superiority of conjugate gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from block.iterative import ConjGrad\n",
    "\n",
    "riesz = riesz_H10\n",
    "\n",
    "dim = 2\n",
    "for n in (8, 16, 32, 64, 128, 256, 512):\n",
    "    A, b, V = poisson(dim, n)\n",
    "    B = riesz(V)\n",
    "    \n",
    "    uh = Function(V)\n",
    "    x = uh.vector()\n",
    "    # Start from random initial guess\n",
    "    x.set_local(np.random.rand(x.local_size())) \n",
    "    \n",
    "    Ainv = ConjGrad(A=A, precond=B, initial_guess=x, tolerance=1e-4)\n",
    "    x[:] = Ainv*b\n",
    "    niters = len(Ainv.residuals)-1\n",
    "    \n",
    "    print 'n = %d, dofs = %d, niters = %d' % (n, A.size(0), niters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- How do the CG and steepest descent algorithms perform with some simpler preconditioner or even without any preconditioner?\n",
    "- Changing the preconditioner for something else, e.g. Jacobi, Gauss-Seidel, see [here](https://bitbucket.org/fenics-apps/cbc.block/src/8447b459156459b5a6f29ce8129fc07c2dc644cd/block/algebraic/petsc/precond.py?at=master&fileviewer=file-view-default) in all the tests below is a useful experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Iterative solvers for convection-diffusion\n",
    "\n",
    "In most text books exposition of iterative methods typically revolves around symmetric, positive-definite matrices. We break away from this tradition and introduce asymmetry into our Poisson problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cd_system_Galerkin(n, epsilon=1):\n",
    "    '''\n",
    "    Linear system corresponding to P1 discretization of convection-diffusion problem\n",
    "    \n",
    "        -\\epsilon \\Delta u + mag\\beta \\cdot \\nabla u = f\n",
    "                                          u = 0 on inflow\n",
    "                                      du/dn = 0 on outflow\n",
    "    '''\n",
    "    mesh = UnitSquareMesh(n, n)\n",
    "    V = FunctionSpace(mesh, 'CG', 1)\n",
    "    u = TrialFunction(V)\n",
    "    v = TestFunction(V)\n",
    "    \n",
    "    bc = DirichletBC(V, Constant(0), 'near(x[0], 0)')\n",
    "    \n",
    "    # Divergence free wind velocity\n",
    "    beta = Expression(('A*x[1]*(1-x[1])', '0'), degree=2, A=1.)\n",
    "    \n",
    "    f = Constant(1)\n",
    "    \n",
    "    a = Constant(epsilon)*inner(grad(u), grad(v))*dx + inner(dot(beta, grad(u)), v)*dx\n",
    "    L = inner(f, v)*dx\n",
    "    \n",
    "    A = assemble(a)\n",
    "    b = assemble(L)\n",
    "    bc.apply(A, b)\n",
    "    \n",
    "    return A, b, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- The following works with given parameter $\\epsilon$. See how sensitive the method parameter $\\rho$ is with respect to $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riesz = riesz_H1\n",
    "\n",
    "dim = 2\n",
    "for n in (8, 16, 32, 64, 128):\n",
    "    A, b, V = cd_system_Galerkin(n, epsilon=1E0)\n",
    "    B = riesz(V)\n",
    "    \n",
    "    uh = Function(V)\n",
    "    x = uh.vector()\n",
    "    # Start from random initial guess\n",
    "    x.set_local(np.random.rand(x.local_size())) \n",
    "    \n",
    "    # Step size \\rho\n",
    "    rho = 0.8\n",
    "    Ainv = Richardson(A=A, precond=rho*B, initial_guess=x, tolerance=1e-4)\n",
    "    x[:] = Ainv*b\n",
    "    niters = len(Ainv.residuals)-1\n",
    "    \n",
    "    print 'n = %d, dofs = %d, niters = %d' % (n, A.size(0), niters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that experince let's use some Krylov method, where no tuning is required. We can't used conjugate gradients for convection diffusion problem since the problem is not symmetric. The lack of symmetry also excludes minimal residual method (MINRES). We are left with GMRES. The code below should work well with given $\\epsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from block.iterative import LGMRES\n",
    "\n",
    "riesz = riesz_H1\n",
    "\n",
    "dim = 2\n",
    "for n in (8, 16, 32, 64, 128, 256, 512):\n",
    "    A, b, V = cd_system_Galerkin(n, epsilon=1E-3)\n",
    "    B = riesz(V)\n",
    "    \n",
    "    uh = Function(V)\n",
    "    x = uh.vector()\n",
    "    # Start from random initial guess\n",
    "    x.set_local(np.random.rand(x.local_size())) \n",
    "    \n",
    "    # Step size \\rho\n",
    "    Ainv = LGMRES(A=A, precond=B, initial_guess=x, tolerance=1e-6)\n",
    "    x[:] = Ainv*b\n",
    "    niters = len(Ainv.residuals)-1\n",
    "    \n",
    "    print 'n = %d, dofs = %d, niters = %d' % (n, A.size(0), niters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- What happens when you increase the parameter's value? Note that GMRES is memory intense so consider smaller matrices.\n",
    "- To simplify things (forget abour convegence of iterative solver), try switching to direct solver and see how the approximation properties are for different $\\epsilon$. To do this you should implement a simpler ($1d$ is fine) problem for which you will have at your disposal an analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to deal with dominating convection is to introduce diffusion. One possibility is the [Streamline-Upwind-Petrov-Galerkin method](http://www.sciencedirect.com/science/article/pii/0045782582900718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cd_system_SUPG(n, epsilon=1.):\n",
    "    '''\n",
    "    Linear system corresponding to P1-SUPG discretization of convection-diffusion problem\n",
    "    \n",
    "        -\\epsilon \\Delta u + mag\\beta \\cdot \\nabla u = f\n",
    "                                          u = 0 on inflow\n",
    "                                      du/dn = 0 on outflow\n",
    "                                      \n",
    "    and the related Riesz map preconditioner\n",
    "    '''\n",
    "    mesh = UnitSquareMesh(n, n)\n",
    "    V = FunctionSpace(mesh, 'CG', 1)\n",
    "    u = TrialFunction(V)\n",
    "    v = TestFunction(V)\n",
    "    \n",
    "    bc = DirichletBC(V, Constant(0), 'near(x[0], 0)')\n",
    "    \n",
    "    # Divergence free wind velocity\n",
    "    beta = Expression(('x[1]*(1-x[1])', '0'), degree=2)\n",
    "    \n",
    "    f = Constant(1)\n",
    "    \n",
    "    a = Constant(epsilon)*inner(grad(u), grad(v))*dx + inner(dot(beta, grad(u)), v)*dx\n",
    "    L = inner(f, v)*dx\n",
    "    \n",
    "    h = mesh.hmin()\n",
    "    beta_max = abs(0.25)\n",
    "    delta = Constant(h/2./beta_max)\n",
    "    # SUPG Stabilization\n",
    "    a += delta*inner(dot(beta, grad(u)), dot(beta, grad(v)))*dx\n",
    "    L += inner(f, delta*dot(beta, grad(v)))*dx\n",
    "    \n",
    "    A = assemble(a)\n",
    "    b = assemble(L)\n",
    "    bc.apply(A, b)\n",
    "    \n",
    "    # -----\n",
    "    \n",
    "    a = Constant(epsilon)*inner(grad(u), grad(v))*dx +\\\n",
    "        delta*inner(dot(beta, grad(u)), dot(beta, grad(v)))*dx\n",
    "    L = inner(Constant(0), v)*dx\n",
    "\n",
    "    B, _ = assemble_system(a, L, bc)\n",
    "    assert A.size(0) == A.size(1) == B.size(0) == B.size(1) == b.size()\n",
    "    B = AMG(B)\n",
    "    \n",
    "    return A, b, B, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we verify that $h$ robustness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in (8, 16, 32, 64, 128, 256, 512):\n",
    "    A, b, B, V = cd_system_SUPG(n, epsilon=1E-4)\n",
    "    \n",
    "    uh = Function(V)\n",
    "    x = uh.vector()\n",
    "    # Start from random initial guess\n",
    "    x.set_local(np.random.rand(x.local_size())) \n",
    "    \n",
    "    # Step size \\rho\n",
    "    Ainv = LGMRES(A=A, precond=B, initial_guess=x, tolerance=1e-4)\n",
    "    x[:] = Ainv*b\n",
    "    niters = len(Ainv.residuals)-1\n",
    "    \n",
    "    print 'n = %d, dofs = %d, niters = %d' % (n, A.size(0), niters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise**\n",
    "- How are the iterations effected by changes in $\\epsilon$. You might want to consult [3]\n",
    "- And what about approximation properties?**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please report to me any 'bugs' you find in the above code. They are left there for purpose (secret features) and I want to make sure they are all uncovered ;)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
